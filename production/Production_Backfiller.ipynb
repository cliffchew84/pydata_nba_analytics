{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challener Model 1 Backfill + Production\n",
    "- This is useful for creating a backtesting framework. For now, I want to use this as a backfilling framework too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gspread\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "from sklearn import metrics\n",
    "from prod_funs import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_folder = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'First_Production_model.sav'\n",
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropbox Stuff \n",
    "#### Make sure that I have scripted the new data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dropbox_functions import authenticate_into_dropbox\n",
    "from dropbox_functions import load_file_into_my_dropbox, download_file_from_dropbox\n",
    "account = authenticate_into_dropbox()\n",
    "files_to_shift = ['game_date_2017.csv', 'main_players_2017.csv', 'more_home_away_2017.csv', 'more_team_stats_2017.csv',\n",
    "                  'more_wins_losses_2017.csv', 'referees_2017.csv', 'start_bench_2017.csv', 'team_2017.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from existing season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check and reload data to remove duplicates\n",
    "for i in glob.glob(\"*2017.csv\"):\n",
    "    tmp = pd.read_csv(i)\n",
    "    tmp.drop_duplicates().to_csv(i, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_table = pd.DataFrame()\n",
    "home_csv, teams_csv, more_stats_csv, game_date_csv, win_loss_csv = load_all_files(\"2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing this season's games for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_home_away = create_home_away_var(home_csv).drop_duplicates()\n",
    "merge_date = pd.merge(merge_home_away, create_date_variable(game_date_csv))\n",
    "merge_team_stats = add_game_stats(teams_csv.drop_duplicates(), merge_date)\n",
    "\n",
    "merge_game_count = create_game_count_var(merge_team_stats)\n",
    "merge_previous_date = create_days_from_previous_games_var(merge_game_count)\n",
    "merge_opp = create_opp_stats(merge_previous_date)\n",
    "merge_opp = create_win_loss_vars(merge_opp)\n",
    "\"\"\" I need to cumulate all numeric stats \"\"\"\n",
    "var_to_accum = merge_opp.drop([\"GAME_ID\", \"TEAM_ID_x\", \"TEAM_ID_y\", \"TEAM_NAME_x\", \"TEAM_NAME_y\", \"G_x\", \"G_y\", \n",
    "                               \"Home\", \"GAME_DATE\", \"WL_x\", 'p_games_x', 'p_games_y', \"SEASON\", \"LIVE_PERIOD\"], axis=1).columns\n",
    "    \n",
    "\"\"\" Potential for change: Right now, I am using the entire season for the stats calculations. \n",
    "    However, it might be more accurate to use rolling window calculations instead.\"\"\"\n",
    "# Create accumulative variables\n",
    "merge_total = merge_opp.sort_values([\"TEAM_NAME_x\", \"GAME_DATE\"])\n",
    "merge_total[var_to_accum] = merge_total.groupby('TEAM_ID_x')[var_to_accum].transform(pd.Series.cumsum)\n",
    "\n",
    "# Create NBA domain-knowledge variables\n",
    "merge_total = create_rebs_efficiency_vars(create_fta_to_fga_ratio(create_efg_var(merge_total)))\n",
    "\n",
    "# Create percentage variables \n",
    "\"\"\"Dividing the cumulative stats of these variables by the games_played thus far to get their averages.\"\"\"\n",
    "vars_to_average = [\"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"W\", \"L\"]\n",
    "merge_total_ave = create_shooting_percentage_vars(create_averages(merge_total, vars_to_average))\n",
    "# merge_games_shifted = shift_game_stats_down_by_one(merge_total_ave)\n",
    "final_t_ab_opp = create_team_ab_and_opp_table(merge_total_ave)\n",
    "finals_home = filter_home_teams(final_t_ab_opp)\n",
    "final_table = final_table.append(finals_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add player statistics inside!\n",
    "- **Good model for future data inclusion!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players = pd.read_csv(\"main_players_2017.csv\")\n",
    "players_past_stats = pd.read_csv(\"player_season_statistics.csv\")\n",
    "players_past_stats = players_past_stats[players_past_stats.season==2016][[\n",
    "    \"PLAYER_ID\", \"PLAYER_NAME\", \"defenders\", \"facilitator\", \"game_winners\", \"inside_gamers\", \"pure_scorers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_players = pd.merge(players, players_past_stats, on='PLAYER_ID', how='left').groupby(['GAME_ID', \"TEAM_ID\"])[\n",
    "    \"defenders\", \"facilitator\", \"game_winners\", \"inside_gamers\", \"pure_scorers\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_table = pd.merge(pd.merge(final_table, team_players.rename(columns={\n",
    "    \"GAME_ID\":\"game_id\", 'TEAM_ID':'team_id_ta', 'defenders':'defenders_a', 'facilitator':'facilitator_a', \n",
    "    'game_winners':'game_winners_a', 'inside_gamers':'insider_gamers_a', 'pure_scorers':'pure_scorers_a'}), \n",
    "                  on=['game_id', 'team_id_ta'], how='left'), team_players.rename(columns={\n",
    "    \"GAME_ID\":\"game_id\", 'TEAM_ID':'team_id_tb', 'defenders':'defenders_b', 'facilitator':'facilitator_b', \n",
    "    'game_winners':'game_winners_b', 'inside_gamers':'insider_gamers_b', 'pure_scorers':'pure_scorers_b'}), \n",
    "         on=['game_id', 'team_id_tb'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for predictions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ta_table = final_table.copy()\n",
    "ta_table = ta_table[['game_id', ] + [i for i in final_table.columns if '_ta' in i] + \n",
    "                    ['defenders_a', 'facilitator_a', 'game_winners_a', 'insider_gamers_a', 'pure_scorers_a']]\n",
    "\n",
    "tb_table = final_table.copy()\n",
    "tb_table = tb_table[['game_id', ] + [i for i in final_table.columns if '_ta' in i] + \n",
    "                    ['defenders_b', 'facilitator_b', 'game_winners_b', 'insider_gamers_b', 'pure_scorers_b']]\n",
    "tb_table.columns = [i.replace(\"_ta_opp\", \"_tb_opp\").replace(\"_ta\", \"_tb\") for i in tb_table.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_predict_table(predict_table, table, ta_table, tb_table):\n",
    "    # Create team_names for merging\n",
    "    predict_table[\"team_name_ta\"] = [i.split(\" \")[-1] for i in predict_table['home']]\n",
    "    predict_table['team_name_ta'].replace(\"Blazers\", \"Trail Blazers\", inplace=True)\n",
    "    predict_table[\"team_name_tb\"] = [i.split(\" \")[-1] for i in predict_table['away']]\n",
    "    predict_table['team_name_tb'].replace(\"Blazers\", \"Trail Blazers\", inplace=True)\n",
    "\n",
    "    team_a_latest_stats = ta_table[ta_table[\"team_name_ta\"].isin(predict_table['team_name_ta'].unique().tolist())].sort_values(\n",
    "        [\"game_id\", 'team_name_ta'], ascending=[False, False]).drop_duplicates(['team_name_ta'])\n",
    "    team_b_latest_stats = tb_table[tb_table[\"team_name_tb\"].isin(predict_table['team_name_tb'].unique().tolist())].sort_values(\n",
    "        [\"game_id\", 'team_name_tb'], ascending=[False, False]).drop_duplicates(['team_name_tb'])\n",
    "\n",
    "    predict_table = pd.merge(\n",
    "        predict_table, \n",
    "        team_a_latest_stats[[i for i in final_table.columns if \"_ta\" in i] + [i for i in final_table.columns if \"_la\" in i]], \n",
    "        on='team_name_ta', how='left')\n",
    "\n",
    "    predict_table = pd.merge(\n",
    "        predict_table, \n",
    "        team_b_latest_stats[[i for i in final_table.columns if \"_tb\" in i] + [i for i in final_table.columns if \"_lb\" in i]], \n",
    "        on='team_name_tb', how='left')\n",
    "\n",
    "    predict_table[\"pts_ast_ta\"] = predict_table['pts_ta'] / predict_table['ast_ta']\n",
    "    predict_table[\"pts_ast_tb\"] = predict_table['pts_tb'] / predict_table['ast_tb']\n",
    "\n",
    "    predict_table[\"pts_ast_ta_opp\"] = predict_table['pts_ta'] / predict_table['ast_ta']\n",
    "    predict_table[\"pts_ast_tb_opp\"] = predict_table['pts_tb'] / predict_table['ast_tb']\n",
    "\n",
    "    predict_table['game_win_rates_ta'] = predict_table[\"w_rate_ta\"] * predict_table['g_ta'] \n",
    "    predict_table['game_win_rates_tb'] = predict_table[\"w_rate_tb\"] * predict_table['g_tb']\n",
    "    return predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_predictions(predictions, model):\n",
    "    final_predict = for_prediction(predictions, model)\n",
    "\n",
    "    proba = pd.DataFrame(loaded_model.predict_proba(final_predict))\n",
    "    proba.columns = ['home_w', \"phome_w\"]\n",
    "    proba.loc[proba[\"phome_w\"] > .5, \"home_w\"] = 1\n",
    "    proba.loc[proba[\"phome_w\"] <= .5, \"home_w\"] = 0\n",
    "\n",
    "    results = pd.concat([predictions[[\"away\", \"home\", \"arena\", \"location\", \"time\", \"d_filter\"]], proba], axis=1)\n",
    "    results['home_w'] = results['home_w'].astype(int)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def today_games_results(table, game_date_result):\n",
    "    win_loss = table.copy()\n",
    "    win_loss['game_date'] = pd.to_datetime(win_loss['GAME_DATE_EST'])\n",
    "    win_loss = win_loss[win_loss.game_date == game_date_result][[\"GAME_ID\", \"TEAM_NICKNAME\", \"PTS\", \"game_date\"]]\n",
    "    win_loss = pd.merge(win_loss.groupby(['GAME_ID']).first().reset_index(), win_loss.groupby(['GAME_ID']).last().reset_index(), \n",
    "             on=[\"GAME_ID\", \"game_date\"])\n",
    "    win_loss.columns = ['game_id', 'team_name_ta', 'pts_ta', 'game_date', 'team_name_tb', 'pts_tb']\n",
    "    win_loss = win_loss[['game_id', 'game_date', 'team_name_ta', 'team_name_tb', 'pts_ta', 'pts_tb']]\n",
    "    win_loss[\"predictions\"] = 0\n",
    "    win_loss.loc[(win_loss['pts_ta'] > win_loss['pts_tb']), \"predictions\"] = 1\n",
    "    return win_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_prediction(dataframe, var):\n",
    "    \"\"\"This functions selects the variables required and make them into sklearn-ready formats! \"\"\"\n",
    "    # hackish way to create up the dataframe, haha... \n",
    "    dataframe['wl_ta'] = 1\n",
    "    y, x = dmatrices('wl_ta ~ ' + var, dataframe, return_type=\"dataframe\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parameters of the best model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_9_ad_players = '''\n",
    "game_win_rates_ta + game_win_rates_tb + g_ta + g_tb + p_games_ta + p_games_tb + \n",
    "pts_ast_ta + pts_ast_tb + pts_ast_ta_opp + pts_ast_tb_opp + \n",
    "\n",
    "pts_ta + oreb_ta + dreb_ta + ast_ta + stl_ta + blk_ta + to_ta +\n",
    "pts_tb + oreb_tb + dreb_tb + ast_tb + stl_tb + blk_tb + to_tb + \n",
    "pts_ta_opp + oreb_ta_opp + dreb_ta_opp + ast_ta_opp + stl_ta_opp + blk_ta_opp + to_ta_opp +\n",
    "pts_tb_opp + oreb_tb_opp + dreb_tb_opp + ast_tb_opp + stl_tb_opp + blk_tb_opp + to_tb_opp +\n",
    "\n",
    "efg_ta + fgp_ta + efg_ta_opp + fgp_ta_opp + fta_fga_ta + fta_fga_ta_opp + fg3p_ta + ftp_ta + \n",
    "efg_tb + fgp_tb + efg_tb_opp + fgp_tb_opp + fta_fga_tb + fta_fga_tb_opp + fg3p_tb + ftp_tb\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for predictions run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_table['game_id'] = final_table['game_id'].astype(int)\n",
    "schedule = pd.read_csv(\"2017_2018_nba_schedule.csv\")\n",
    "schedule['d_filter'] = [datetime.datetime.strptime(i, \"%A, %B %d\").strftime(\"%d-%m\") for i in schedule['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of dates to backfill! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime.date(year=2017, month=10, day=23)\n",
    "end_date = (datetime.datetime.now() - datetime.timedelta(days=1)).date()\n",
    "start_date, end_date\n",
    "date_list = pd.date_range(start=start_date, end=end_date)\n",
    "len(date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load new day games for today's predictions\n",
    "- **Looping through the different days start from here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy for 2017-10-23: 0.625\n",
      "Prediction Accuracy for 2017-10-24: 0.833333333333\n",
      "Prediction Accuracy for 2017-10-25: 0.8\n",
      "Prediction Accuracy for 2017-10-26: 0.8\n",
      "Prediction Accuracy for 2017-10-27: 0.857142857143\n",
      "Prediction Accuracy for 2017-10-28: 0.5\n",
      "Prediction Accuracy for 2017-10-29: 0.428571428571\n",
      "Prediction Accuracy for 2017-10-30: 0.555555555556\n",
      "Prediction Accuracy for 2017-10-31: 0.5\n",
      "Prediction Accuracy for 2017-11-01: 0.5\n",
      "Prediction Accuracy for 2017-11-02: 1.0\n",
      "Prediction Accuracy for 2017-11-03: 0.833333333333\n",
      "Prediction Accuracy for 2017-11-04: 0.8\n",
      "Prediction Accuracy for 2017-11-05: 0.7\n",
      "Prediction Accuracy for 2017-11-06: 0.666666666667\n",
      "Prediction Accuracy for 2017-11-07: 0.5\n",
      "Prediction Accuracy for 2017-11-08: 0.8\n",
      "Prediction Accuracy for 2017-11-09: 0.8\n",
      "Prediction Accuracy for 2017-11-10: 0.5\n",
      "Prediction Accuracy for 2017-11-11: 0.909090909091\n",
      "Prediction Accuracy for 2017-11-12: 1.0\n",
      "Prediction Accuracy for 2017-11-13: 0.666666666667\n",
      "Prediction Accuracy for 2017-11-14: 0.666666666667\n",
      "Prediction Accuracy for 2017-11-15: 0.818181818182\n",
      "Prediction Accuracy for 2017-11-16: 1.0\n",
      "Prediction Accuracy for 2017-11-17: 0.727272727273\n",
      "Prediction Accuracy for 2017-11-18: 0.857142857143\n",
      "Prediction Accuracy for 2017-11-19: 0.666666666667\n",
      "Prediction Accuracy for 2017-11-20: 0.727272727273\n",
      "Prediction Accuracy for 2017-11-21: 1.0\n",
      "Prediction Accuracy for 2017-11-22: 0.5\n",
      "2017-11-23 had no games!\n",
      "Prediction Accuracy for 2017-11-24: 0.6\n",
      "Prediction Accuracy for 2017-11-25: 0.8\n",
      "Prediction Accuracy for 2017-11-26: 0.666666666667\n",
      "Prediction Accuracy for 2017-11-27: 0.625\n",
      "Prediction Accuracy for 2017-11-28: 0.6\n",
      "Prediction Accuracy for 2017-11-29: 0.7\n",
      "Prediction Accuracy for 2017-11-30: 0.6\n",
      "Prediction Accuracy for 2017-12-01: 0.875\n",
      "Prediction Accuracy for 2017-12-02: 0.75\n",
      "Prediction Accuracy for 2017-12-03: 0.8\n",
      "Prediction Accuracy for 2017-12-04: 0.636363636364\n",
      "Prediction Accuracy for 2017-12-05: 0.666666666667\n",
      "Prediction Accuracy for 2017-12-06: 1.0\n",
      "Prediction Accuracy for 2017-12-07: 0.5\n",
      "Prediction Accuracy for 2017-12-08: 0.5\n",
      "Prediction Accuracy for 2017-12-09: 0.6\n",
      "Prediction Accuracy for 2017-12-10: 1.0\n",
      "Prediction Accuracy for 2017-12-11: 0.5\n",
      "Prediction Accuracy for 2017-12-12: 0.428571428571\n",
      "Prediction Accuracy for 2017-12-13: 0.555555555556\n",
      "Prediction Accuracy for 2017-12-14: 1.0\n",
      "Prediction Accuracy for 2017-12-15: 0.545454545455\n",
      "Prediction Accuracy for 2017-12-16: 0.875\n",
      "Prediction Accuracy for 2017-12-17: 0.5\n",
      "Prediction Accuracy for 2017-12-18: 0.7\n",
      "Prediction Accuracy for 2017-12-19: 0.666666666667\n",
      "Prediction Accuracy for 2017-12-20: 0.583333333333\n",
      "Prediction Accuracy for 2017-12-21: 0.6\n",
      "2017-12-22 had no games!\n"
     ]
    }
   ],
   "source": [
    "for date2predict in date_list:\n",
    "    try:\n",
    "        d_filter = date2predict.strftime(\"%d-%m\")\n",
    "        predictions = schedule[schedule['d_filter'] == d_filter]\n",
    "\n",
    "        today = final_table[final_table.game_date == date2predict]\n",
    "        today_predict_table = making_predict_table(predictions, today, ta_table, tb_table)\n",
    "        today_predict = making_predictions(today_predict_table, model_9_ad_players)\n",
    "\n",
    "        today_games = today_games_results(win_loss_csv, date2predict)\n",
    "        today_predict['team_name_ta'] = [i.split(\" \")[-1] for i in today_predict[\"home\"]]\n",
    "        today_predict['team_name_tb'] = [i.split(\" \")[-1] for i in today_predict[\"away\"]]\n",
    "        today_predict['team_name_ta'].replace(\"Blazers\", \"Trail Blazers\", inplace=True)\n",
    "        today_predict['team_name_tb'].replace(\"Blazers\", \"Trail Blazers\", inplace=True)\n",
    "\n",
    "        ffinal = pd.merge(today_predict, today_games, on=['team_name_ta', 'team_name_tb'], how='left')\n",
    "        ffinal = ffinal[[\"away\", 'home', 'location', 'home_w', \"phome_w\", 'predictions', 'd_filter', 'pts_ta', 'pts_tb']]\n",
    "        ffinal['p_score'] = 1\n",
    "        ffinal.loc[ffinal['predictions'] != ffinal['home_w'], 'p_score'] = 0\n",
    "        print(\"Prediction Accuracy for {}: {}\".format(date2predict.date(), ffinal['p_score'].sum() / float(len(ffinal))))\n",
    "        # ffinal.to_csv(\"predict/cm1/cm1_{}_predictions.csv\".format(date2predict), index=False)\n",
    "    except:\n",
    "        print(\"{} had no games!\".format(date2predict.date()))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
