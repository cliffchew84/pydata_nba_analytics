{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created on: 2017-11-05 15:14 \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print \"Created on: {} \".format(datetime.datetime.now().strftime('%Y-%m-%d %H:%M'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twelve: Some quick feature engineerings\n",
    "As I am working on some player statistics, I decided to test some experiments in feature engineering, basically use variable differences and some ratios to see if they can help reduce the number of variables in my logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gspread\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_folder = os.path.abspath(os.getcwd())\n",
    "os.chdir(\".\")\n",
    "db_folder = os.getcwd() + \"/new_data\"\n",
    "os.chdir(db_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('API Project-f22fe0b03992.json', scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading games data from entry 9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33801, 114)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = pd.read_csv(\"1986_2016_seasons_shifted_v1.csv\")\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "Let's review some of the basic statistics again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   0.61\n",
       "0   0.39\n",
       "Name: wl_ta, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.wl_ta.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating difference features...\n",
    "I have no theoretical basis for these new features. I just want increase the interaction between my many number of variables, and hopefully reduce the number of variables before moving on to some more interesting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']:\n",
    "    main[\"{}_d\".format(i)] = main['{}_ta'.format(i)] - main['{}_tb'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']:\n",
    "    main[\"{}_ta_d\".format(i)] = main['{}_ta'.format(i)] - main['{}_ta_opp'.format(i)]\n",
    "    \n",
    "for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']:\n",
    "    main[\"{}_tb_d\".format(i)] = main['{}_tb'.format(i)] - main['{}_tb_opp'.format(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading model results to Google Drive\n",
    "- Again, I am tracking my mode prediction here. I purposely left an empty row in my results to remind myself that the results are found in a separate entry. https://docs.google.com/spreadsheets/d/1qKpRuE3imqWAzNXnUDvClN-nlb0bUSLyb24AuYO0V0w/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc = gspread.authorize(credentials)\n",
    "dashboard = gc.open(\"Tracking NBA Prediction Models\").worksheet(\"Logistic Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_scores(row, model_name, model, model_specs, x, y, x_, y_):\n",
    "    row_model = dashboard.find(str(row))._row\n",
    "    model_inputs = dashboard.range('A{}:K{}'.format(row_model, row_model))\n",
    "    y_pred = model.predict(x_)\n",
    "    values = [row_model - 1, model_name,\n",
    "              model.score(x, y), \n",
    "              model.score(x_, y_), \n",
    "              metrics.recall_score(y_, y_pred), \n",
    "              metrics.precision_score(y_, y_pred),\n",
    "              metrics.f1_score(y_, y_pred), \n",
    "              metrics.roc_auc_score(y_, y_pred),\n",
    "              metrics.log_loss(y_, y_pred),\n",
    "              str(datetime.datetime.now()), \"\".join(model_specs.splitlines())]\n",
    "    \n",
    "    for cell, value in zip(model_inputs, values):\n",
    "        cell.value = value        \n",
    "    \n",
    "    return dashboard.update_cells(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** wl_ta ** is the variable dependent variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the framework for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_analysis(dataframe, var):\n",
    "    \"\"\"This functions selects the variables required and make them into sklearn-ready formats! \"\"\"\n",
    "    y, x = dmatrices('wl_ta ~ ' + var, dataframe, return_type=\"dataframe\")\n",
    "    y = np.ravel(y)\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Model 9 with difference variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_9_diff_vars = '''\n",
    "w_rate_ta * g_ta + w_rate_tb * g_tb + g_ta + g_tb + p_games_ta + p_games_tb + \n",
    "pts_ta + oreb_ta + dreb_ta + ast_ta + stl_ta + blk_ta + to_ta +\n",
    "pts_tb + oreb_tb + dreb_tb + ast_tb + stl_tb + blk_tb + to_tb + \n",
    "pts_ta_opp + oreb_ta_opp + dreb_ta_opp + ast_ta_opp + stl_ta_opp + blk_ta_opp + to_ta_opp +\n",
    "pts_tb_opp + oreb_tb_opp + dreb_tb_opp + ast_tb_opp + stl_tb_opp + blk_tb_opp + to_tb_opp +\n",
    "\n",
    "pts_ta_d + oreb_ta_d + dreb_ta_d + ast_ta_d + stl_ta_d + blk_ta_d + to_ta_d +\n",
    "pts_tb_d + oreb_tb_d + dreb_tb_d + ast_tb_d + stl_tb_d + blk_tb_d + to_tb_d + \n",
    "\n",
    "pts_d + oreb_d + dreb_d + ast_d + stl_d + blk_d + to_d + \n",
    "\n",
    "efg_ta + fgp_ta + efg_ta_opp + fgp_ta_opp + fta_fga_ta + fta_fga_ta_opp + fg3p_ta + ftp_ta + \n",
    "efg_tb + fgp_tb + efg_tb_opp + fgp_tb_opp + fta_fga_tb + fta_fga_tb_opp + fg3p_tb + ftp_tb +\n",
    "C(team_id_ta)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_test_advanced, x_test_advanced = for_analysis(main[main.season == main.season.max()], model_9_diff_vars)\n",
    "# y_train_advanced, x_train_advanced = for_analysis(main[(main.season < main.season.max())], model_9_diff_vars)\n",
    "\n",
    "# reg_logit = LogisticRegression(random_state=1984, C=0.01)\n",
    "# reg_logit.fit(x_train_advanced, y_train_advanced)\n",
    "\n",
    "# test_data_scores(15, \"Mod_9+diff_vars\", \n",
    "#                  model=reg_logit, model_specs = model_9_diff_vars, \n",
    "#                  x=x_train_advanced, y=y_train_advanced,\n",
    "#                  x_=x_test_advanced, y_=y_test_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ratio variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main[\"pts_ast_ta\"] = main['pts_ta'] / main['ast_ta']\n",
    "main[\"pts_ast_tb\"] = main['pts_tb'] / main['ast_tb']\n",
    "\n",
    "main[\"pts_ast_ta_opp\"] = main['pts_ta'] / main['ast_ta']\n",
    "main[\"pts_ast_tb_opp\"] = main['pts_tb'] / main['ast_tb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the model... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main['game_win_rates_ta'] = main[\"w_rate_ta\"] * main['g_ta'] \n",
    "main['game_win_rates_tb'] = main[\"w_rate_tb\"] * main['g_tb'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Model 9 with difference variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_9_ratio_vars = '''\n",
    "game_win_rates_ta + game_win_rates_tb + g_ta + g_tb + p_games_ta + p_games_tb + \n",
    "pts_ast_ta + pts_ast_tb + pts_ast_ta_opp + pts_ast_tb_opp + \n",
    "\n",
    "pts_ta + oreb_ta + dreb_ta + ast_ta + stl_ta + blk_ta + to_ta +\n",
    "pts_tb + oreb_tb + dreb_tb + ast_tb + stl_tb + blk_tb + to_tb + \n",
    "pts_ta_opp + oreb_ta_opp + dreb_ta_opp + ast_ta_opp + stl_ta_opp + blk_ta_opp + to_ta_opp +\n",
    "pts_tb_opp + oreb_tb_opp + dreb_tb_opp + ast_tb_opp + stl_tb_opp + blk_tb_opp + to_tb_opp +\n",
    "\n",
    "efg_ta + fgp_ta + efg_ta_opp + fgp_ta_opp + fta_fga_ta + fta_fga_ta_opp + fg3p_ta + ftp_ta + \n",
    "efg_tb + fgp_tb + efg_tb_opp + fgp_tb_opp + fta_fga_tb + fta_fga_tb_opp + fg3p_tb + ftp_tb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_advanced, x_test_advanced = for_analysis(main, model_9_ratio_vars)\n",
    "y_train_advanced, x_train_advanced = for_analysis(main, model_9_ratio_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1984, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_logit = LogisticRegression(random_state=1984, C=0.01)\n",
    "reg_logit.fit(x_train_advanced, y_train_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scores(26, \"Full_Mod_9+ratio_vars\", \n",
    "                 model=reg_logit, model_specs = model_9_ratio_vars, \n",
    "                 x=x_train_advanced, y=y_train_advanced,\n",
    "                 x_=x_test_advanced, y_=y_test_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are clear cut worst then before. **My best model is still model 9 for now!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Step: Saving model reuse!\n",
    "- I am adding a new work process as well, which is to always save my latest best model for 'deployment'. This will allow me to load my model anywhere without the need to re-train it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cliff/cliffchew84.github.io/notebooks/new_data'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First_production_model.sav']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'First_production_model.sav'\n",
    "joblib.dump(reg_logit, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
